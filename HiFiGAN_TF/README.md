# README

Description: HiFi-GAN is a GAN based vocoder used with neural text to speech models.

### Notes:

 * Tensors in Pytorch would be shaped (batch_size, channels, height, width) while tensors in Tensorflow are shaped (batch_size, height, width, channels). The channels dim/axis is what is operated on by each framework's respective layers so there is a need to transpose the tensors in this tensorflow implementation (I cannot vouch for its correctness but I can say that the model is able to build & run).


### Notes

 * Changes from the [Original Repo](https://github.com/jik876/hifi-gan):
     * The training and validation files are specified via the arguments passed into `train.py`. I swapped that out for the setup that I have in GradTTS & FastPitch, where the files are in a folder called `filelists/` and read from there.
     * `mel_dataset.py` is an attempt to emulate the original data loading function from PyTorch. Given that the original uses `scipy.io.wavfile.read()` to read in the audio data, there will be some slight differences in the data when compared to using tensorflow to read in the audio (as shown in the `AudioProcessing/` folder). Thus, there isn't really a reason to use it over the `data.py` module that's been create and used across the other audio models.
 * Tried to add a `build()` call for the models (generator and discriminators) but found it quite cumbersome. For that reason, I've also created methods to initialize the models via Tensorflow's functional API. I dont like it compared to sub-classing `tf.keras.Model` but the models can be built immediately after initialization.
     * Additionally, I found an issue when trying to initialize the mpd discriminator model, there seemed to be an issue with the shape of the tensor (because the DiscriminatorP layer breaks down the inputs into its base dimensions).
 * In `data.py`, the `Data` class is the main data loader for the HiFi GAN model. The old data loader (`DataOld` class) was based on the data loaders for the text to speech models (GradTTS, FastPitch, Tacotron2), but was insufficient as it only returned the mel spectrograms and the mel spectrogram lengths for each sample. The new data loader is returning the input mel spectrogram, the audio, file name, and the mel loss mel spectrogram. There are also several caveats with the new data loader as well.
     * `mel_loss` is the mel spectrograms generated from tacotron 2 as a part of teacher-forcing for fine-tuning only. In training, this would probably be the same as `fmax`.
     * `fmax_mel_loss` is the `fmax` for preprocessing the mel loss mel spectrogram.
     * The new data loader is much faster than the old data loader despite not having the ability to cache processed data to (.npy) files.
         * The old data loader required two passes through the data. One to acquire the max length of the mel spectrograms and the second pass was to pad the data to that length accordingly. On a cold start, this required 10 minutes for each pass, leading to a total of 20 minutes to process the training data. The processed mel spectrograms were saved to (.npy) files as a sort of caching mechanism to reduce the amount of time spent in processing the audio. This cut down warm start times to 2.5 minutes for the initial pass and 7 minutes for the second pass, resulting in 10 minutes for total data processing (reduced time by 50%).
         * The new data load requires only one pass through the data, but cannot take advantage of caching with files due to randomness in the workflow. In addition, the audio is sub-sampled or padded accordingly to fit a set length (`segment_size`) meaning that there is no need to dedicate an entire pass through the data to find the max lengths. This also means that not all the audio data is being processed if it is larger than that `segment_size`. These attributes greatly reduced the time needed to load the data to a `tf.data.Dataset` to 7 minutes every time, regardless of warm start or cold start.
     * `base_mels_path` is another variable used for the folder containing the mel spectrograms generated by tacotron 2. It is only a concern for fine-tuning HiFi GAN and not really for training. 
 * Additions I had to make for the model architecture & training:
     * `mel_g_hat` has its length padded after coming out of the generator in training. This is due to the generator not necessarily generating tensors of the same length as the input `y` tensor (With current configurations, input `x` mel length is 29 and `y` audio length is 8192). The padding was also necessary because the discriminators, which take in `y` and `mel_g_hat` as inputs require the tensors to be the same shape (especially the length dimensions).
     * The `stft.mel_spectrogram()` function was written to only process one input at a time. When it is called again at training to process the `mel_g_hat`, that tensor contains an extra dimension for the batch. To deal with this, the tensor is un-stacked and each entry in the batch is process through the function before being stacked together. This occurs after the padding of the `mel_g_hat`.
     * Initially, I used ChatGPT to generate custom layers for the `weight_norm` and `spectral_norm` functions from pytorch. Then I found out about the `WeightNormalization` ([documentation](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/WeightNormalization)) and `SpectralNormalization` ([documenation](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization)) from tensorflow-addons. I replaced the ChatGPT implementations with these layers.
         * Whether these actually allow me to save the model or not I've yet to test.
         * It was important to set the `data_init` argument in the `WeightNormalization` layer from tensorflow-addons to `False` because the default was `True` and that wasn't correct. However, in the lines where `SpectralNormalization` and `WeightNormalization` could be interchanged depending on the arguments passed into the parent layer (see `DiscriminatorP` and `DiscriminatorS` layers), it was difficult to specify that argument. Given that `SpectralNormalization` and `WeightNormalization` both have a second argument that is predefined, I used the same logic that sets which normalization layer to use to set the second argument of the respective layer.
         * When running training, the mel spectrograms created by the generator are passed through the `mel_spectrogram()` function to pass to the mel loss. When in graph execution mode, tensorflow throws an error because it does not like using tensorflow operations (such as `tf.shape()`) as part of a python condition check (such as `if` or `assert` statements). An extra argument was added to the `mel_spectrogram()` that is set by default in favor of the data loader. That argument is set in the `train_step()` and `test_step()` to allow to bypass those checks so that the error does not occur when running the model in graph execution.
     * The convolution layers (usually Conv2D) where padding is a tuple of int values (some of which are called from `get_padding()`) are replaced with the "same" string. This is because none of the convolution layers in tensorflow accept tuples for the padding. "same" was also chosen because the first values in the padding tuples usually resolved to a value greater than 1, which translates to "same" padding in tensorflow.


### TODO List (for V1 release)

 1. Finish Model architecture
     * ~~Iron out the padding for the conv layers~~
     * Go back and add weight initializer for respective layers
     * Implement ~~initialization, training loop,~~ model saving/loading, and ~~compiling~~ in `gan.py`. Alternatively, implement everything in `train.py`
 2. ~~Implement data loading~~
 3. Implement everything required for training (this can be done either in `gan.py` or `train.py`)
     * ~~Model initialization~~
     * ~~Model compilation~~
         * ~~Model loss~~
         * ~~Model optimizer~~
     * ~~Training loop~~
     * Model saving/loading (includes resume saving from epoch)