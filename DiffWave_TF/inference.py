# inference.py


from argparse import ArgumentParser
import os
import numpy as np
import tensorflow as tf
from common.audio_processing_tf import STFT
from data import load_wav_to_tensorflow
from params import params
from model import DiffWave


def predict_by_slice(model, audio, T, spectrogram):
	# Split the audio & spectrogram data into slices. This is because
	# the model was trained on smaller slices (spectrogram had a length
	# of params.crop_mel_frames (62) and audio had a length of
	# params.hop_length x params.crop_mel_frames for the conditional
	# model).
	split_length = params.crop_mel_frames * params.hop_length
	tensor_length = audio.shape[-1]

	whole_count = tensor_length // split_length
	remainder = tensor_length % split_length

	audio_slices = tf.split(
		audio[:, :split_length * whole_count], whole_count, axis=1,
	)
	spectrogram_slices = tf.split(
		spectrogram[:, :params.crop_mel_frames * whole_count, :],
		whole_count, axis=1
	)

	if remainder != 0:
		audio_slices.append(
			tf.slice(
				audio, begin=[0, split_length * whole_count], 
				size=[1, remainder]
			)
		)
		spectrogram_slices.append(
			tf.slice(
				spectrogram, 
				begin=[0, params.crop_mel_frames * whole_count, 0],
				size=[1, remainder // params.hop_length, params.n_mels]
			)
		)

	# Number of audio slices must match number of spectrogram slices.
	assert len(audio_slices) == len(spectrogram_slices)

	# Iterate through each slice and have the model perform inference.
	# Concatenate the results to form a tensor of the same size as the
	# input audio tensor.
	output_slices = []
	for audio_slice, spectrogram_slice in zip(audio_slices, spectrogram_slices):
		output_slices.append(
			model([audio_slice, T, spectrogram_slice])
		)
	output = tf.concat(output_slices, axis=1)

	return output


# @tf.function
def predict(spectrogram=None, model_dir=None, params=None, 
		fast_sampling=False):
	if model_dir is None:
		raise ValueError(f"model_dir must be specified")

	# Load model from model_dir.
	model = tf.keras.models.load_model(model_dir)

	# Initialize noise schedule.
	training_noise_schedule = np.array(params.noise_schedule)
	inference_noise_schedule = np.array(
		params.inference_noise_schedule
	) if fast_sampling else training_noise_schedule

	# Change in notation from the DiffWave paper for fast sampling.
	# DiffWave paper -> Implementation below
	# --------------------------------------
	# alpha -> talpha
	# beta -> training_noise_schedule
	# gamma -> alpha
	# eta -> beta
	talpha = 1 - training_noise_schedule
	talpha_cum = np.cumprod(talpha)

	beta = inference_noise_schedule
	alpha = 1 - beta
	alpha_cum = np.cumprod(alpha)

	T = []
	for s in range(len(inference_noise_schedule)):
		for t in range(len(training_noise_schedule) - 1):
			if talpha_cum[t + 1] <= alpha_cum[s] <= talpha_cum[t]:
				twiddle = (
					talpha_cum[t] ** 0.5 - alpha_cum[s] ** 0.5
				) / (talpha_cum[t] ** 0.5 - talpha_cum[t + 1] ** 0.5)
				T.append(t + twiddle)
				break
	T = np.array(T, dtype=np.float32)

	if not params.unconditional:
		if len(spectrogram.shape) == 2:# Expand rank 2 tensors by adding a batch dimension.
			# spectrogram = spectrogram.unsqueeze(0) # Original
			spectrogram = tf.expand_dims(spectrogram, 0)
		# audio = torch.randn(spectrogram.shape[0], model.params.hop_samples * spectrogram.shape[-1]) # Original
		audio = tf.random.normal(
			(
				spectrogram.shape[0],							# batch_size
				params.hop_length * spectrogram.shape[1],		# audio length
			)
		)
	else:
		# audio = torch.randn(1, params.audio_len) # Original
		audio = tf.random.normal((1, params.audio_len))
	# noise_scale = torch.from_numpy(alpha_cum**0.5).float().unsqueeze(1) # Original
	noise_scale = tf.expand_dims(
		tf.convert_to_tensor(alpha_cum ** 0.5, dtype=tf.float32), -1
	)

	for n in range(len(alpha) - 1, -1, -1):
		c1 = 1 / alpha[n] ** 0.5
		c2 = beta[n] / (1 - alpha_cum[n]) ** 0.5
		# audio = c1 * (audio - c2 * model(audio, torch.tensor([T[n]]), spectrogram).squeeze(1)) # Original
		audio = c1 * (
			audio - c2 * tf.squeeze(
				model(
					[audio, tf.convert_to_tensor([T[n]]), spectrogram],
					training=False
				), axis=-1
			)
		)
		# audio = c1 * (
		# 	audio - c2 * tf.squeeze(
		# 		predict_by_slice(
		# 			model, audio, tf.convert_to_tensor([T[n]]), spectrogram
		# 		), 
		# 		axis=-1
		# 	)
		# )
		if n > 0:
			# noise = torch.randn_like(audio) # Original
			noise = tf.random.uniform((audio.shape))
			sigma = ((1.0 - alpha_cum[n - 1]) /
				(1.0 - alpha_cum[n]) * beta[n]) ** 0.5
			audio += sigma * noise
		# audio = torch.clamp(audio, -1.0, 1.0) # Original
		audio = tf.clip_by_value(audio, -1.0, 1.0)

	return audio, params.sample_rate


# @tf.function
def main():
	parser = ArgumentParser(description='runs inference on a spectrogram file generated by diffwave.preprocess')
	# parser.add_argument('model_dir',
	# 	help='directory containing a trained model (or full path to weights.pt file)')
	parser.add_argument('--spectrogram_path', '-s',
		help='path to a spectrogram file generated by diffwave.preprocess')
	parser.add_argument('--output', '-o', default='output.wav',
		help='output file name')
	parser.add_argument('--fast', '-f', action='store_true',
		help='fast sampling procedure')
	args = parser.parse_args()

	if args.spectrogram_path:
		spectrogram = tf.convert_to_tensor(
			np.load(args.spectrogram_path)
		)
	else:
		# spectrogram = None
		max_wav_value = 32768.0
		stft = STFT(
			filter_length=params.n_fft, 
			frame_step=params.hop_length,
			frame_length=params.win_length, 
			sampling_rate=params.sample_rate,
			mel_fmin=params.f_min, mel_fmax=params.f_max
		)
		filename = './LJSpeech-1.1/wavs/LJ001-0001.wav'

		audio, sampling_rate = load_wav_to_tensorflow(filename)
		audio_norm = audio / max_wav_value
		audio_norm = tf.expand_dims(audio_norm, 0)
		spectrogram = stft.mel_spectrogram(audio_norm)

	model_dir = "./diff_wave"

	audio, sr = predict(
		spectrogram, model_dir, fast_sampling=args.fast,
		params=params
	)
	tf.io.write_file(
		args.output, tf.audio.encode_wav(
			# tf.expand_dims(tf.squeeze(audio, 0), -1), sr
			tf.transpose(audio), sr
		)
	)

	# Exit the program.
	exit(0)


if __name__ == '__main__':
	# with tf.device('/cpu:0'):
	# 	main()
	main()