# README

Description: Grad-TTS is a diffusion based neural text to speech model.

### Notes:

 * The original model was trained on a single Nvidia RTX 2080 Ti (11GB VRAM) for 1.7 million iterations (batch size 16). Current training speed on my 2060 SUPER (8GB VRAM) with the same batch size is ~10 epochs an hour (or 240 epochs/day). Given how there are 746 iterations per epoch, that means there needs to be 2,275 epochs (1.7M iters X 746 iters/epoch X 240 epochs/day) or rather 9.5 days of training on this device.
 * `inference.py` only processes text from a file one line at a time. There is no allowance for batched processing at the moment.
 * Tensors in Pytorch would be shaped (batch_size, channels, height, width) while tensors in Tensorflow are shaped (batch_size, height, width, channels). The channels dim/axis is what is operated on by each framework's respective layers so there is a need to transpose the tensors in this tensorflow implementation (I cannot vouch for its correctness but I can say that the model is able to build & run).
 * Graph execution vs Eager execution
     * A Tensorflow model can be compiled in eager execution model by setting the `run_eagerly` argument to `True` in `model.compile()`. By default, `run_eagerly=False` or `None`. See the Tensorflow `tf.keras.Model` [documentation](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/Model#compile) for more information.
     * Eager execution evaluates operations immediate while Graph execution builds a computational graph. Eager execution was added to Tensorflow 2.0 to compete with Pytorch, which uses eager execution by default. Eager execution also allows for easier debugging (and makes it more appealing to beginners). Graph execution extracts the tensor computations from Python and converts it to an efficient graph. This can be seen in Tensorflow 2.0 by wrapping an eager executing code/function with a `tf.function` header. For more information on eager vs graph execution in Tensorflow 2, see [this](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6) Medium article as well as [this](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html) Tensorflow blog post.
     * The (LJSpeech) dataset is initialized in the `data_function.py` under the `Data` class. The dataset is initialized as a `tf.data.dataset` by using the `from_generator()` function ([documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)). When the model is compiled in Eager execution mode, the `output_signature` shapes can be defined with `None`. However, the shapes *must* be defined explicity for all shapes (meaning that `None` cannot be in any of the shape values). To avoid this, compute the maximum length for the respective elements of the dataset and save that as a class file to be accessed later.
     * Another caveat to (LJSpeech) dataset initialization is that there is a way to initialize the dataset with the `from_tensor_slices()` function ([documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)). Using `from_generator()` allows the dataset to be loaded dynamically while `from_tensor_slices()` loads everything to memory. See the linked documentation for any caveats ([Building Tensorflow input pipelines with tf.data](https://www.tensorflow.org/guide/data)). Note that multiple examples on the Keras.io website ([ASR using CTC](https://keras.io/examples/audio/ctc_asr/), [ASR using Transformers](https://keras.io/examples/audio/transformer_asr/), [Speaker Recognition](https://keras.io/examples/audio/transformer_asr/)) deal with audio data use the `tf.data.dataset.from_tensor_slices()` function. Currently, loading the dataset with `from_tensor_slices()` is not fully operational in this folder/model. The `data.tensor_slices()` function generates a list of tuples of tensors but there are some errors when called from the `tf.data.dataset.from_tensor_slices()` function.
     * Currently, despite my best attempts to get the model to run in both Graph execution and Eager execution mode, the monotonic align module relies on numpy (and there are other areas in the model rely on numpy calculations because numpy arrays are mutable). This reliance causes issues with converting tensorflow tensors to numpy arrays. In Eager execution there is no issue converting a tensorflow tensor to numpy arrays, but there are issues in Graph execution mode (the model becomes more complex along with the logic). This model will only support Eager execution mode (will have to see how this affects model saving & loading for TF-Lite & TFJS).


### TODO List (for V1 release)

 1. ~~Verify/Implement model call() function (used in inference) despite having loss function running in train_step.~~
 2. ~~Remove all print debut statements.~~
 3. ~~Attempt to run in graph execution mode (this will also require some changes to data loading. Can reference the changes made for FastPitch data loading).~~ Update: Unable to fully run the model in graph execution mode. I could try and go back and try and rewrite the monotonic_align module to use 100% tensorflow (along with the other numpy uses in the model) but that would be after the whole project is finished.
 4. Implement & train HiFi-GAN network for vocoder.
 5. Rework loss/compute_loss. Seeing a warning message at every step (not epoch, step) that gradients do not exist for variables.
 6. Rework build() & summary(). We want to be able to build the model so that we can call model.summary()
 7. Rework call(). This is for converting model to TFLite or TFJS (I donâ€™t know how passing a dict will affect the conversion process).
 8. Implement model checkpointing during training. This also covers model saving/loading as well as resuming training from a previous epoch.

Overall, it seems like sub-classed tf.keras.Model models are very hard to work with (compared to Sequential or Functional models). Then again, it does make sense to use the sub-classed Model because of the custom training steps vs inference steps along with the custom loss functions.