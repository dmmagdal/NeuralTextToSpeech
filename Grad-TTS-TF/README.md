# README

Description: Grad-TTS is a diffusion based neural text to speech model.

### Notes:

 * The original model was trained on a single Nvidia RTX 2080 Ti (11GB VRAM) for 1.7 million iterations (batch size 16). Current training speed on my 2060 SUPER (8GB VRAM) with the same batch size is ~10 epochs an hour (or 240 epochs/day). Given how there are 746 iterations per epoch, that means there needs to be 2,275 epochs (1.7M iters X 746 iters/epoch X 240 epochs/day) or rather 9.5 days of training on this device.
 * `inference.py` only processes text from a file one line at a time. There is no allowance for batched processing at the moment.
 * Tensors in Pytorch would be shaped (batch_size, channels, height, width) while tensors in Tensorflow are shaped (batch_size, height, width, channels). The channels dim/axis is what is operated on by each framework's respective layers so there is a need to transpose the tensors in this tensorflow implementation (I cannot vouch for its correctness but I can say that the model is able to build & run).
 * Graph execution vs Eager execution
     * A Tensorflow model can be compiled in eager execution model by setting the `run_eagerly` argument to `True` in `model.compile()`. By default, `run_eagerly=False` or `None`. See the Tensorflow `tf.keras.Model` [documentation](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/Model#compile) for more information.
     * Eager execution evaluates operations immediate while Graph execution builds a computational graph. Eager execution was added to Tensorflow 2.0 to compete with Pytorch, which uses eager execution by default. Eager execution also allows for easier debugging (and makes it more appealing to beginners). Graph execution extracts the tensor computations from Python and converts it to an efficient graph. This can be seen in Tensorflow 2.0 by wrapping an eager executing code/function with a `tf.function` header. For more information on eager vs graph execution in Tensorflow 2, see [this](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6) Medium article.
     * The (LJSpeech) dataset is initialized in the `data_function.py` under the `Data` class. The dataset is initialized as a `tf.data.dataset` by using the `from_generator()` function ([documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)). When the model is compiled in Eager execution mode, the `output_signature` shapes can be defined with `None`. However, the shapes *must* be defined explicity for all shapes (meaning that `None` cannot be in any of the shape values).

     * There is a way to initialize the dataset with the `from_tensor_slices()` function ([documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)) but that will cause OOM on my 8GB GPU (probably from all the extra data from calculating/loading the pitch, energy, & attention prior matrices). Using `from_generator()` allows the dataset to be loaded dynamically while `from_tensor_slices()` loads everything to memory. See the linked documentation for any caveats ([Building Tensorflow input pipelines with tf.data](https://www.tensorflow.org/guide/data)).
 * Compiling the model in eager execution mode (set `run_eagerly=True` in the model's [`compile()`](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/Model#compile) function) will eagerly take the data from the tf.data.dataset (with the specified batch size, especially when the dataset was generated with the `from_generator()` function). This is useful for debugging & tracing the shape and data of tensors throughout the model.
 * Compiling the model in graph execution model (do *not* set the `run_eagerly` parameter in the model's [`compile()`](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/Model#compile) function) can provide optimizations to make the model run faster with better memory efficiency while eager execution simplifies the model building experience (see this [Tensorflow blog post](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html)). We'd like to train the model in graph execution mode for these benefits. One thing to note is that when passing the dataset after initializing it with the `from_generator()` method results in the batch size dimension being None (even when batched & prefetched). Another thing to note about compiling in graph execution, particularly when the dataset is initialized with `from_generator()`, is that the output shapes should be explicitly outlined in the `output_signature` argument of the `from_generator()` function. If the shapes include None (to account for inputs of dynamic length/shape), this may cause errors when the data is passed through the model (the tensor will have None as part of the shape). To avoid this, compute the maximum length for the respective elements of the dataset and save that as a class file to be accessed later.

### TODO List (for V1 release)

 1. ~~Verify/Implement model call() function (used in inference) despite having loss function running in train_step.~~
 2. ~~Remove all print debut statements.~~
 3. Attempt to run in graph execution mode (this will also require some changes to data loading. Can reference the changes made for FastPitch data loading).
 4. Implement & train HiFi-GAN network for vocoder.
 5. Rework loss/compute_loss. Seeing a warning message at every step (not epoch, step) that gradients do not exist for variables.
 6. Rework build() & summary(). We want to be able to build the model so that we can call model.summary()
 7. Rework call(). This is for converting model to TFLite or TFJS (I donâ€™t know how passing a dict will affect the conversion process).
 8. Implement model checkpointing during training. This also covers model saving/loading as well as resuming training from a previous epoch.

Overall, it seems like sub-classed tf.keras.Model models are very hard to work with (compared to Sequential or Functional models). Then again, it does make sense to use the sub-classed Model because of the custom training steps vs inference steps along with the custom loss functions.